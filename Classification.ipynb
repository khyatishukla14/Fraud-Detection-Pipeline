{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Credit Card Fraud Detection  \n",
        "This notebook demonstrates a complete machine learning classification pipeline on an **imbalanced dataset** (credit card fraud detection).  \n",
        "\n",
        "We will:  \n",
        "- Perform exploratory data analysis (EDA)  \n",
        "- Engineer new features  \n",
        "- Build preprocessing and resampling pipelines  \n",
        "- Train and compare multiple models  \n",
        "- Evaluate with proper metrics (Precision, Recall, F1, ROC-AUC)  \n",
        "- Analyze feature importance and model interpretability  \n",
        "\n",
        "Handling class imbalance is especially important here, since fraudulent transactions are very rare compared to non-fraudulent ones.  "
      ],
      "metadata": {
        "id": "eMHlEcrzOKsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0kwvKUREmMg"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score, RocCurveDisplay, ConfusionMatrixDisplay\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Loading Dataset and Overview\n",
        "- The dataset contains credit card transactions.  \n",
        "- The target variable is `Class` (0 = normal, 1 = fraud).  \n",
        "- It is **highly imbalanced**: fraudulent transactions are much lesser.  \n"
      ],
      "metadata": {
        "id": "EXHc7-tjaX0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "7nJ02mGyGPU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Exploratory Data Analysis\n",
        "In this step, I explore the dataset to understand its structure and balance.\n",
        "- The dataset has 284,807 transactions.\n",
        "- Only ~0.17% of them are fraud, which means the dataset is highly imbalanced.\n",
        "- The imbalance can be seen in the countplot below.\n"
      ],
      "metadata": {
        "id": "9KpwCyqxPA01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(df[\"Class\"].value_counts())  # Class = 1 is fraud, 0 is normal\n",
        "\n",
        "# Plot imbalance\n",
        "sns.countplot(x=\"Class\", data=df)\n",
        "plt.title(\"Class Distribution (Imbalance)\")\n",
        "plt.show()\n",
        "\n",
        "# Summary stats\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "uQ6tL5pHGQXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Feature Engineering\n",
        "Creating two new features:\n",
        "1. `Amount_log`: a log-transformed version of transaction amount to reduce skew.\n",
        "2. `Hour`: extract transaction hour from the `Time` column."
      ],
      "metadata": {
        "id": "D3Wu06fpPNXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Amount_log\"] = np.log1p(df[\"Amount\"])\n",
        "df[\"Hour\"] = (df[\"Time\"] // 3600) % 24  # time in hours"
      ],
      "metadata": {
        "id": "i-OMM5pkGlV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Preprocessing\n",
        "- Split data into training and testing sets.\n",
        "- Scale numeric features using StandardScaler (important for SVM and Logistic Regression).\n"
      ],
      "metadata": {
        "id": "FuYo2GLdPZOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"Class\", axis=1)\n",
        "y = df[\"Class\"]\n",
        "\n",
        "# Train-test split (stratify to keep imbalance ratio)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "KLcFYjFZJAir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Handling Class Imbalance\n",
        "We use SMOTE to generate synthetic fraud samples and balance the dataset. This prevents the model from always predicting ‚Äúnot fraud.‚Äù\n"
      ],
      "metadata": {
        "id": "3vnQmkphPdWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "print(\"Resampled dataset shape:\", np.bincount(y_train_resampled))"
      ],
      "metadata": {
        "id": "YnE-j9w1JFFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Model Comparison and Analysis\n",
        "\n",
        "After training and evaluating three models (Logistic Regression, Random Forest, and SVM) on the sampled dataset, I compared their performance using **F1-Score** and **ROC-AUC**.\n",
        "\n",
        "- **Logistic Regression**: Performs well as a baseline, with balanced precision and recall. It is simple, interpretable, and very fast to train.  \n",
        "- **Random Forest**: Achieves higher performance than Logistic Regression by capturing complex, non-linear relationships. It generally shows a better F1-Score and ROC-AUC, meaning it can detect fraud cases more accurately.  \n",
        "- **SVM**: Provides competitive results but is much slower to train on large datasets. Since we sampled the dataset for efficiency, its results are acceptable, but Random Forest usually scales better for larger datasets.\n"
      ],
      "metadata": {
        "id": "BTiTnSlvPfA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take 30% of the dataset for faster training\n",
        "df_small = df.sample(frac=0.3, random_state=42)\n",
        "\n",
        "# Split again into features/target\n",
        "X_small = df_small.drop(\"Class\", axis=1)\n",
        "y_small = df_small[\"Class\"]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_small)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Handle imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Define models (SVM without probability=True for speed)\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"SVM\": SVC()\n",
        "}\n",
        "\n",
        "# Train & evaluate\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "    # ROC-AUC handling\n",
        "    if name == \"SVM\":\n",
        "        y_scores = model.decision_function(X_test_scaled)\n",
        "        print(\"ROC-AUC:\", roc_auc_score(y_test, y_scores))\n",
        "    else:\n",
        "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "        print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "        RocCurveDisplay.from_predictions(y_test, y_proba)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "fVDZQV72JI3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Hyperparameter Tuning\n",
        "We tune Random Forest with GridSearchCV to find the best hyperparameters for improved performance."
      ],
      "metadata": {
        "id": "yEV-GrfBPwSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200],\n",
        "    \"max_depth\": [5, 10, None],\n",
        "}\n",
        "grid = GridSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train_resampled, y_train_resampled)\n",
        "print(\"Best Params:\", grid.best_params_)"
      ],
      "metadata": {
        "id": "nlw6PUgUJNII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Feature Importance\n",
        "We analyze which features contributed most to the Random Forest model.This improves interpretability and helps understand fraud patterns."
      ],
      "metadata": {
        "id": "9mjiWt7xP1y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf = grid.best_estimator_\n",
        "importances = best_rf.feature_importances_\n",
        "\n",
        "feat_importances = pd.Series(importances, index=X.columns)\n",
        "feat_importances.nlargest(10).plot(kind=\"barh\")\n",
        "plt.title(\"Top 10 Important Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "piPkKoqVJTdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Final Evaluation\n",
        "We visualize:\n",
        "- Confusion Matrix\n",
        "- ROC Curve"
      ],
      "metadata": {
        "id": "WF3fSiTYP-Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_estimator(best_rf, X_test_scaled, y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IZV4uGTtJVsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Results:\n",
        "- Logistic Regression: F1 = 0.83, ROC-AUC = 0.95\n",
        "- Random Forest: F1 = 0.89, ROC-AUC = 0.97 (Best)\n",
        "- SVM: F1 = 0.85, ROC-AUC = 0.96\n",
        "\n",
        "### **Conclusion:**\n",
        "- Random Forest is the best-performing model overall, balancing high **F1-Score** and **ROC-AUC** (indicating strong overall discrimination between fraud and non-fraud).\n",
        "- Logistic Regression remains useful as a simple, interpretable baseline, while SVM is less practical for large-scale fraud detection due to training time.\n",
        "- In fraud detection, **Recall** is crucial (catch as many frauds as possible), but we also need **Precision** (to avoid false alarms).\n",
        "- Feature importance shows that features like V14, V17, and Amount_log are especially useful.\n",
        "\n"
      ],
      "metadata": {
        "id": "t2S47LU0QCv0"
      }
    }
  ]
}